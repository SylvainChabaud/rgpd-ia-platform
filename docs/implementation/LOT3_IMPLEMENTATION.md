# LOT 3.0 - Provider IA local POC branch√© √† la Gateway

> **EPIC 3** : Validation technique IA locale (POC contr√¥l√©)
> **Statut** : ‚úÖ Impl√©ment√©
> **Date** : 2025-12-24

---

## 1. Objectifs du LOT 3.0

Valider la faisabilit√© d'un provider IA local en conditions contr√¥l√©es, tout en garantissant une conformit√© RGPD stricte.

### Acceptance Criteria (bloquants)

- [x] Prompts/outputs NON persist√©s par d√©faut
- [x] IA accessible uniquement via la Gateway LLM (pas de bypass)
- [x] Provider local (Ollama) branch√© sur `invokeLLM()`
- [x] Bench latence sur donn√©es fictives uniquement (P0)
- [x] Tests RGPD obligatoires passants

---

## 2. Architecture impl√©ment√©e

### 2.1 Composants cr√©√©s

```
src/ai/gateway/
‚îú‚îÄ‚îÄ config.ts                    # Configuration providers (stub | ollama)
‚îú‚îÄ‚îÄ invokeLLM.ts                 # Gateway LLM avec routing (modifi√©)
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ types.ts                 # Types communs providers
    ‚îú‚îÄ‚îÄ stub.ts                  # Provider stub (existant, inchang√©)
    ‚îî‚îÄ‚îÄ ollama.ts                # Provider Ollama local (nouveau)

docker-compose.dev.yml           # Service Ollama ajout√© (modifi√©)

scripts/
‚îî‚îÄ‚îÄ bench-llm.ts                 # Bench latence minimal (nouveau)

tests/
‚îî‚îÄ‚îÄ rgpd.no-prompt-storage.test.ts  # Test bloquant LOT 3.0 (nouveau)

docs/implementation/
‚îî‚îÄ‚îÄ LOT3_IMPLEMENTATION.md       # Ce document
```

### 2.2 Flux d'appel

```
Frontend / API
      ‚Üì
invokeLLM() [Gateway LLM unique]
      ‚Üì
   config.ts (AI_PROVIDER)
      ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   stub   ‚îÇ  ollama  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì           ‚Üì
   R√©ponse    Ollama local
   fictive    (http://ollama:11434)
```

**Conformit√©** :
- ‚úÖ Gateway unique ([BOUNDARIES.md](../architecture/BOUNDARIES.md))
- ‚úÖ Aucun bypass possible (test `rgpd.no-llm-bypass.test.ts`)
- ‚úÖ Providers stateless (aucun stockage)

---

## 3. Provider Ollama (local POC)

### 3.1 Caract√©ristiques

- **Image** : `ollama/ollama:latest`
- **R√©seau** : `rgpd_internal` (isol√©)
- **Ports** : NON expos√©s (acc√®s interne uniquement)
- **Mod√®le** : `tinyllama` (l√©ger, POC)
- **Volume** : `ollama_data` (persistance mod√®les uniquement)

### 3.2 Contraintes RGPD

**Conformit√© stricte** :
- ‚ùå AUCUN stockage prompts/outputs ([LLM_USAGE_POLICY.md](../ai/LLM_USAGE_POLICY.md) ¬ß6)
- ‚ùå AUCUN log de contenu (√©v√©nements uniquement)
- ‚úÖ Donn√©es fictives uniquement (P0)
- ‚úÖ Isolation r√©seau compl√®te (pas de flux externe)
- ‚úÖ Stateless (pas d'historique, pas de cache)

### 3.3 Configuration

Variables d'environnement (`.env` ou `docker-compose.dev.yml`) :

```bash
# Provider actif (stub | ollama)
AI_PROVIDER=ollama

# Ollama configuration
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=tinyllama
OLLAMA_TIMEOUT=30000
```

**Valeurs par d√©faut** :
- `AI_PROVIDER=stub` (fallback s√©curis√©)
- `OLLAMA_URL=http://localhost:11434`
- `OLLAMA_MODEL=tinyllama`

---

## 4. Tests RGPD (LOT 3.0)

### 4.1 Tests obligatoires (bloquants)

| Test | Fichier | Statut | Conformit√© |
|------|---------|--------|-----------|
| No prompt storage | `rgpd.no-prompt-storage.test.ts` | ‚úÖ Passant | LLM_USAGE_POLICY ¬ß6 |
| No LLM bypass | `rgpd.no-llm-bypass.test.ts` | ‚úÖ Passant | BOUNDARIES.md ¬ß6 |
| No sensitive logs | `rgpd.no-sensitive-logs.test.ts` | ‚úÖ Passant | DATA_CLASSIFICATION ¬ß7 |

### 4.2 Test "No Prompt Storage" (nouveau)

**Objectif** : V√©rifier que AUCUN prompt ou output n'est persist√©.

**Assertions** :
1. ‚úÖ Aucun pattern de stockage DB dans le code source
2. ‚úÖ Invocation LLM ne persiste rien
3. ‚úÖ Pas de cache entre invocations
4. ‚úÖ Pas de table DB pour prompts/outputs

**Couverture** :
- Scan statique des patterns interdits (`INSERT INTO prompts`, `.create({ prompt: ... })`)
- Test runtime (double invocation, pas de cache)
- Validation sch√©ma DB (pas de tables prompts/outputs)

---

## 5. Bench latence minimal

### 5.1 Usage

**Pr√©requis** : Ollama en cours d'ex√©cution (Docker ou local)

```bash
# Lancer Ollama (Docker)
docker compose -f docker-compose.dev.yml up -d ollama

# T√©l√©charger le mod√®le (premi√®re fois uniquement)
docker exec rgpd-platform-ollama-poc ollama pull tinyllama

# Ex√©cuter le bench
AI_PROVIDER=ollama tsx scripts/bench-llm.ts
```

**Avec provider stub** (tests CI) :

```bash
AI_PROVIDER=stub tsx scripts/bench-llm.ts
```

### 5.2 Prompts fictifs (P0 uniquement)

Le bench utilise **10 prompts fictifs** (donn√©es publiques, non personnelles, non sensibles) :

- R√©sum√© de texte Lorem Ipsum
- Cat√©gorisation de documents types
- Extraction de champs structur√©s fictifs
- Reformulation de texte technique g√©n√©rique
- D√©tection de type de document
- Normalisation de processus
- Suggestions UI g√©n√©riques
- Classification de demandes
- Identification d'entit√©s fictives
- R√©sum√© d'articles techniques

**Conformit√©** :
- ‚úÖ P0 uniquement ([DATA_CLASSIFICATION.md](../data/DATA_CLASSIFICATION.md))
- ‚úÖ Aucune donn√©e r√©elle, personnelle ou sensible
- ‚úÖ R√©sultats NON persist√©s (console uniquement)

### 5.3 M√©triques mesur√©es

- **Latence** : Min, P50, Avg, P95, P99, Max
- **Tokens** : Total input, total output, moyennes

**Exemple de sortie** :

```
=== LLM Latency Benchmark (LOT 3.0 POC) ===
Provider: ollama
Prompts: 10 fictitious prompts

[1/10] Invoking LLM...
  ‚úì Latency: 1.23s
  ‚úì Tokens: 42 in / 58 out

...

=== Benchmark Results ===
Successful invocations: 10/10

Latency statistics:
  Min:  852ms
  P50:  1.15s
  Avg:  1.23s
  P95:  1.89s
  P99:  2.01s
  Max:  2.01s

Token statistics:
  Total input tokens:  385
  Total output tokens: 612
  Avg input tokens:    39
  Avg output tokens:   61

‚úì Benchmark completed
```

---

## 6. Setup d√©veloppement local

### 6.1 D√©marrage complet (Docker)

```bash
# Lancer l'infra compl√®te (PostgreSQL + Ollama + App)
docker compose -f docker-compose.dev.yml up -d

# V√©rifier les services
docker compose -f docker-compose.dev.yml ps

# Logs Ollama
docker logs rgpd-platform-ollama-poc

# Healthcheck Ollama
docker exec rgpd-platform-ollama-poc curl -f http://localhost:11434/api/tags
```

### 6.2 T√©l√©chargement du mod√®le

**Premi√®re utilisation uniquement** :

```bash
# T√©l√©charger tinyllama (l√©ger, ~800MB)
docker exec rgpd-platform-ollama-poc ollama pull tinyllama

# V√©rifier les mod√®les install√©s
docker exec rgpd-platform-ollama-poc ollama list
```

**Mod√®les alternatifs** (POC) :

- `phi` : Microsoft Phi (1.3B params)
- `gemma:2b` : Google Gemma 2B
- `qwen2:1.5b` : Qwen 1.5B

**‚ö†Ô∏è Attention** : Mod√®les plus lourds (llama3, mistral) non recommand√©s pour POC (ressources CPU/RAM).

### 6.3 Test manuel

```bash
# Depuis l'app Next.js (dans le container)
docker exec -it rgpd-platform-app-dev sh

# Ou en local (si Ollama expos√©)
curl http://localhost:11434/api/generate -d '{
  "model": "tinyllama",
  "prompt": "Hello, this is a test",
  "stream": false
}'
```

---

## 7. Conformit√© RGPD (validation)

### 7.1 Definition of Done (CLAUDE.md ¬ß7)

- [x] Fronti√®res d'architecture respect√©es (Gateway seule entr√©e)
- [x] Aucun appel IA hors Gateway LLM (test valid√©)
- [x] Aucune donn√©e sensible en logs (test valid√©)
- [x] Classification donn√©es respect√©e (P0 uniquement dans bench)
- [x] Tests fonctionnels passants (provider Ollama OK)
- [x] Tests RGPD passants (no-prompt-storage OK)
- [x] Comportement √©chec d√©fini (timeout, error handling)
- [x] Fonctionnalit√© valid√©e (bench latence OK)
- [x] Tra√ßabilit√© RGPD minimale (events only, pas de contenu)

### 7.2 Checklist LLM_USAGE_POLICY.md ¬ß10

- [x] Usage autoris√© (POC contr√¥l√©, transformation/classification)
- [x] Mod√®le local privil√©gi√© (Ollama)
- [x] Gateway utilis√©e (routing via `invokeLLM()`)
- [x] Donn√©es minimis√©es (prompts fictifs P0)
- [x] Pas de logs sensibles (events uniquement)
- [x] Tests ajout√©s (no-prompt-storage)

### 7.3 Preuves de conformit√©

**Artefacts produits** (auditables) :

1. ‚úÖ Tests RGPD passants (49 + 4 = 53 tests)
2. ‚úÖ Scan statique anti-bypass (`rgpd.no-llm-bypass.test.ts`)
3. ‚úÖ Scan patterns stockage (`rgpd.no-prompt-storage.test.ts`)
4. ‚úÖ Bench latence sur donn√©es fictives (console output)
5. ‚úÖ Documentation normative ([BOUNDARIES.md](../architecture/BOUNDARIES.md), [LLM_USAGE_POLICY.md](../ai/LLM_USAGE_POLICY.md))

---

## 8. Limitations & points de vigilance

### 8.1 Limitations POC

- ‚ö†Ô∏è **Mod√®le l√©ger** : tinyllama a des capacit√©s limit√©es (POC uniquement)
- ‚ö†Ô∏è **Pas de GPU** : Inf√©rence CPU uniquement (latences plus √©lev√©es)
- ‚ö†Ô∏è **Pas de streaming** : R√©ponses compl√®tes uniquement (POC)
- ‚ö†Ô∏è **Pas de redaction** : Gateway ne fait PAS encore de redaction active (LOT futur)

### 8.2 Risques identifi√©s & mitigations

| Risque | Mitigation |
|--------|-----------|
| Fuite r√©seau Ollama | ‚úÖ Ports NON expos√©s, r√©seau interne uniquement |
| Stockage involontaire prompts | ‚úÖ Test bloquant + scan statique |
| Logs sensibles | ‚úÖ Test existant `no-sensitive-logs` |
| Donn√©es r√©elles en bench | ‚úÖ P0 uniquement, donn√©es fictives strictes |
| D√©pendance Ollama prod | ‚úÖ Stub reste fallback, config explicite |

### 8.3 Points de vigilance op√©rationnels

- üìå **Ressources** : Ollama n√©cessite ~2GB RAM + CPU pour tinyllama
- üìå **Premier d√©marrage** : T√©l√©chargement mod√®le (~800MB) requis
- üìå **Healthcheck** : 60s start_period (Ollama peut √™tre lent au boot)
- üìå **Logs** : Ollama g√©n√®re des logs techniques (pas de logs m√©tier)

---

## 9. √âvolutions futures (hors LOT 3.0)

### LOT 4.0+ (Stockage RGPD)

- [ ] Impl√©menter table `ai_jobs` (m√©tadonn√©es uniquement, pas de contenu)
- [ ] Valider que test "no prompt storage" reste passant
- [ ] Politique de r√©tention stricte (si stockage m√©tadonn√©es)

### LOT 5.0+ (Pipeline RGPD)

- [ ] Int√©grer Gateway avec enforcement consentement
- [ ] Journalisation audit events IA (sans contenu)

### LOT 6.0+ (Production-ready)

- [ ] √âvaluer mod√®les locaux plus performants (avec GPU)
- [ ] Impl√©menter streaming (si requis)
- [ ] Redaction active dans Gateway (avant envoi provider)
- [ ] Kill switch LLM (circuit breaker)

---

## 10. Commandes utiles

### Docker

```bash
# D√©marrer uniquement Ollama
docker compose -f docker-compose.dev.yml up -d ollama

# Arr√™ter tout
docker compose -f docker-compose.dev.yml down

# Logs en temps r√©el
docker logs -f rgpd-platform-ollama-poc

# Supprimer les volumes (reset complet)
docker compose -f docker-compose.dev.yml down -v
```

### Tests

```bash
# Tous les tests (49 + 4 = 53)
npm test

# Test sp√©cifique LOT 3.0
npm test rgpd.no-prompt-storage

# Test anti-bypass
npm test rgpd.no-llm-bypass
```

### Bench

```bash
# Bench avec Ollama (local)
AI_PROVIDER=ollama tsx scripts/bench-llm.ts

# Bench avec stub (CI)
AI_PROVIDER=stub tsx scripts/bench-llm.ts
```

---

## 11. R√©f√©rences

### Documents normatifs

- [TASKS.md](../../TASKS.md) ‚Äî LOT 3.0 (lignes 290-312)
- [CLAUDE.md](../../CLAUDE.md) ‚Äî R√®gles d√©veloppement IA
- [BOUNDARIES.md](../architecture/BOUNDARIES.md) ‚Äî Fronti√®res d'architecture
- [LLM_USAGE_POLICY.md](../ai/LLM_USAGE_POLICY.md) ‚Äî Politique d'usage LLM
- [DATA_CLASSIFICATION.md](../data/DATA_CLASSIFICATION.md) ‚Äî Classification donn√©es
- [RGPD_TESTING.md](../testing/RGPD_TESTING.md) ‚Äî Tests RGPD

### Impl√©mentations

- [src/ai/gateway/invokeLLM.ts](../../src/ai/gateway/invokeLLM.ts) ‚Äî Gateway LLM
- [src/ai/gateway/providers/ollama.ts](../../src/ai/gateway/providers/ollama.ts) ‚Äî Provider Ollama
- [src/ai/gateway/config.ts](../../src/ai/gateway/config.ts) ‚Äî Configuration
- [tests/rgpd.no-prompt-storage.test.ts](../../tests/rgpd.no-prompt-storage.test.ts) ‚Äî Test bloquant
- [scripts/bench-llm.ts](../../scripts/bench-llm.ts) ‚Äî Bench latence

### Documentation externe

- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Ollama Docker](https://hub.docker.com/r/ollama/ollama)

---

**Document produit dans le cadre du LOT 3.0 ‚Äî EPIC 3 : Validation technique IA locale (POC contr√¥l√©)**

**Conformit√©** : ‚úÖ FULL RGPD
